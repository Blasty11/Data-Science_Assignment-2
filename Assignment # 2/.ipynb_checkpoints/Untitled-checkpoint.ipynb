{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfc7216-242f-4b51-9492-3a5c27b59a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Electricity Demand Data Analysis and Processing\n",
    "\n",
    "This project involves integrating multiple CSV and JSON files containing electricity demand and weather data. The objectives \n",
    "are to clean and preprocess the data, detect and handle outliers, perform exploratory data analysis (EDA), and build a regression \n",
    "model to predict electricity demand. The final deliverables are a cleaned CSV file and this Jupyter Notebook documenting the entire process.\n",
    "\n",
    "## Environment Setup\n",
    "\n",
    "The following libraries are required for this project:\n",
    "- pandas\n",
    "- matplotlib\n",
    "- seaborn\n",
    "- scikit-learn\n",
    "- statsmodels\n",
    "- json\n",
    "\n",
    "You can install these using pip:\n",
    "\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis, zscore\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "\n",
    "## Data Integration\n",
    "\n",
    "The project data is split into multiple CSV files (weather data) and JSON files (electricity demand data). The following code merges \n",
    "these files into single CSV files for further processing.\n",
    "\n",
    "\n",
    "# Function to merge CSV files\n",
    "def merge_csv_files(directory):\n",
    "    all_files = [f for f in os.listdir(directory) if f.endswith('.csv')]\n",
    "    df_list = [pd.read_csv(os.path.join(directory, file)) for file in all_files]\n",
    "    merged_df = pd.concat(df_list, ignore_index=True)\n",
    "    return merged_df\n",
    "\n",
    "# Function to merge JSON files\n",
    "def merge_json_files(directory):\n",
    "    all_files = [f for f in os.listdir(directory) if f.endswith('.json')]\n",
    "    df_list = []\n",
    "    for file in all_files:\n",
    "        with open(os.path.join(directory, file), 'r') as f:\n",
    "            data = json.load(f)\n",
    "            # Adjust the following line based on your JSON structure\n",
    "            df = pd.DataFrame(data[\"response\"][\"data\"])\n",
    "            df_list.append(df)\n",
    "    merged_df = pd.concat(df_list, ignore_index=True)\n",
    "    return merged_df\n",
    "\n",
    "# Merge the files (update paths as needed)\n",
    "csv_directory = \"path/to/csv_files\"\n",
    "json_directory = \"path/to/json_files\"\n",
    "weather_df = merge_csv_files(csv_directory)\n",
    "electricity_df = merge_json_files(json_directory)\n",
    "\n",
    "# Combine the merged CSV and JSON data\n",
    "combined_df = pd.concat([weather_df, electricity_df], axis=1)\n",
    "\n",
    "\n",
    "## Data Preprocessing and Cleaning\n",
    "\n",
    "In this step, we:\n",
    "- Drop columns with more than 50% missing values.\n",
    "- Fill missing values using median imputation for numeric columns and mode imputation for categorical columns.\n",
    "- Convert the PERIOD column to a datetime format and extract features such as hour, day, month, etc.\n",
    "- Remove duplicate rows.\n",
    "\n",
    "\n",
    "def preprocess_data(df):\n",
    "    # Drop columns with >50% missing values\n",
    "    df = df.dropna(thresh=len(df)*0.5, axis=1)\n",
    "    \n",
    "    # Fill missing values\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'object':\n",
    "            df[col].fillna(df[col].mode()[0], inplace=True)\n",
    "        else:\n",
    "            df[col].fillna(df[col].median(), inplace=True)\n",
    "    \n",
    "    # Convert 'period' to datetime and extract features\n",
    "    if 'period' in df.columns:\n",
    "        df['period'] = pd.to_datetime(df['period'], errors='coerce').ffill()\n",
    "        df['period'] = df['period'].astype('datetime64[ns]')\n",
    "        df['hour'] = df['period'].dt.hour\n",
    "        df['day'] = df['period'].dt.day\n",
    "        df['month'] = df['period'].dt.month\n",
    "        df['year'] = df['period'].dt.year\n",
    "        df['day_of_week'] = df['period'].dt.dayofweek\n",
    "        df['is_weekend'] = df['day_of_week'].apply(lambda x: 1 if x >= 5 else 0)\n",
    "    \n",
    "    # Remove duplicate rows\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    return df\n",
    "\n",
    "cleaned_df = preprocess_data(combined_df)\n",
    "\n",
    "\n",
    "## Exploratory Data Analysis (EDA)\n",
    "\n",
    "Below are some visualizations and statistical summaries to understand the distribution, relationships, and trends in the data.\n",
    "\n",
    "\n",
    "# Statistical Summary\n",
    "cleaned_df.describe()\n",
    "\n",
    "# Histogram of key features\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.histplot(cleaned_df['value'], kde=True)\n",
    "plt.title(\"Distribution of Electricity Demand\")\n",
    "plt.xlabel(\"Electricity Demand\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n",
    "# Time Series Plot (if applicable)\n",
    "if 'period' in cleaned_df.columns:\n",
    "    plt.figure(figsize=(12,6))\n",
    "    plt.plot(cleaned_df['period'], cleaned_df['value'])\n",
    "    plt.title(\"Electricity Demand Over Time\")\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Demand\")\n",
    "    plt.show()\n",
    "\n",
    "# Correlation Heatmap\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.heatmap(cleaned_df.corr(), annot=True, cmap='coolwarm')\n",
    "plt.title(\"Correlation Heatmap\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "## Regression Modeling\n",
    "\n",
    "We use a linear regression model to predict electricity demand based on time-based features extracted from the 'period' column. The dataset is split into training and testing sets, and the model's performance is evaluated using Mean Squared Error (MSE), Root Mean Squared Error (RMSE), and RÂ² score.\n",
    "\n",
    "# Select features and target\n",
    "features = ['hour', 'day', 'month', 'day_of_week', 'is_weekend']\n",
    "X = cleaned_df[features]\n",
    "y = cleaned_df['value']\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build and train model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = mse ** 0.5\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"MSE:\", mse)\n",
    "print(\"RMSE:\", rmse)\n",
    "print(\"R2 Score:\", r2)\n",
    "\n",
    "\n",
    "## Final Submission\n",
    "\n",
    "The final deliverables for this project are:\n",
    "- **final_cleaned_data.csv:** The cleaned and processed dataset.\n",
    "- **project.ipynb:** This Jupyter Notebook documenting the entire data processing, analysis, and modeling workflow.\n",
    "\n",
    "These files are submitted as part of the project.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
